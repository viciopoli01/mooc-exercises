{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/dtlogo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython import display\n",
    "import cv2\n",
    "import copy\n",
    "from tqdm.contrib import tzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Particle filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robots use sensors to estimate its state. However, as we know, sensors are noisy and thus should not be fully trusted. The particle filter allows us to estimate state by tracking some set of likely states and updating these as we get new measurements. This is achieved using a set of particles, where each particle $i$ represents a hypothesis of the state state $x_t^i$ with its associated weight $w_t^i$. So for example, in the context of state estimation (e.g., localization), each particle represents how likely the robot is located at a particular state.\n",
    "\n",
    "The algorithm is as follows. First, we initialize the particle filter by sampling $M$ particles. Usually the initial set of particles can be sampled uniformly from the set of all possible states (i.e., uniform sampling). Given a control signal $u_t$, we move all the particles (i.e., the state associated with each particle) that we currently have following the motion model. After this, we take a measurement $z_t$ (which we can get from our sensor) and update the weights of each particle based on how likely we are to measure $z_t$ if we were at the corresponding state of each particle (e.g., $p(z_t | x_i)$). We then resample all the particles, not uniformly, but according to the weights of each particle (i.e., the ones with higher weights are more likely to be sampled). This means that the particles that have higher weights will be sampled more often, while the ones with low weights may not be sampled at all. There are different options to obtain the predicted (i.e., filtered) state. In this notebook, we will do the simplest approach by taking the mean of the resampled particles.\n",
    "\n",
    "We then repeat this process everytime we have new $u_t$ and $z_t$. In practice, we may not know what $p(z_t|x_i)$ is, so we may need other ways to compute the particle weight. We will see this later in the implementation.\n",
    "\n",
    "The above algorithm is the basic algorithm for the particle filter. There are some associated challenges and different algorithm variations try to address these issues. For example, if we implement the above algorithm, the performance will be largely affected by the initial set of particles because during resampling step, we only sample particles from the set of initial particles. Thus, it is possible that we may not even have particles that are near the correct state. This problem is sometimes known as particle depletion. To help alleviate this problem a little bit, during resampling step, instead of sampling completely from the set of particles that we currently have according to their weights, we can sample a small percentage of particles randomly from the set of all possible particles. In this notebook, we will stick with this approach since it is the simplest to implement and allows us to understand the basics of particle filter.\n",
    "\n",
    "Let's take a look at a concrete example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: estimating robot position with particle filter**\n",
    "\n",
    "Consider a robot moving in a room without obstacles. The robot is equipped with two sensors to measure distance between the robot and the walls, which allows the robot to measure the location of the robot (i.e., $x$ and $y$ positions) in the room. For the sake of simplicity, assume that the sensors directly provide us with a noisy measurement of $(x,y)$ location in the room.\n",
    "\n",
    "Say the state of the robot is its $x$ and $y$ position in the room, and the control inputs are the velocity in each direction $v_x$ and $v_y$. The robot is initialized at $(x,y) = (0,0)$, and moves by applying constant control inputs $v_x = v_y = 0.1$ for 100 time steps. At each time step, after applying a control signal, the robot can take a measurement using the sensors to have an idea where it currently is. \n",
    "\n",
    "Given the motion model:\n",
    "\n",
    "$$\n",
    "x_t = x_{t-1} + u_t + w_t,\n",
    "$$\n",
    "\n",
    "where $w_t \\sim \\mathcal{N}(0,0.01)$\n",
    "\n",
    "we will use the particle filter to improve our estimate of where the robot is at each time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the problem**\n",
    "\n",
    "To understand the problem, let us plot the ideal trajectory and some possible measurements that we would get using the available sensors according to their specifications. In real life, we can get sensor measurements from our sensors. But here, to simulate sensor measurements, we will take the ground truth state at every time step and add some Gaussian noise from $\\mathcal{N}(0, 0.5)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = [x_pos, y_pos]\n",
    "num_data = 100\n",
    "ground_truth_x = np.linspace(0, 10, num=num_data + 1)\n",
    "ground_truth_y = ground_truth_x.copy() # x = y\n",
    "    \n",
    "# Simulate sensor measurements\n",
    "measurement_noise_x_var = 0.5\n",
    "measurement_noise_y_var = 0.5\n",
    "noise_x = np.random.normal(loc=0.0, scale=measurement_noise_x_var, size=num_data-1)\n",
    "noise_y = np.random.normal(loc=0.0, scale=measurement_noise_y_var, size=num_data-1)\n",
    "measurement_x = np.linspace(10 / num_data, 10, num=num_data-1) + noise_x\n",
    "measurement_y = np.linspace(10 / num_data, 10, num=num_data-1) + noise_y\n",
    "\n",
    "# Compare ground truth and measurements\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(ground_truth_x, ground_truth_y)\n",
    "plt.plot(measurement_x, measurement_y)\n",
    "plt.xlabel('x position')\n",
    "plt.ylabel('y position')\n",
    "plt.legend(['ground truth trajectory', 'localization measurements from noisy sensor'])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there is quite a lot of noise coming from the sensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing particle filter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a Particle class. Each particle has its state and weight. It also has a method to move itself given a control signal $u_t$ (i.e., predict step) and a method to update its own weight given a measurement $z_t$. Since we do not know what $p(z_t|s_i)$ is, we compute the weight of each particle based on how far it is from the measurement. Intuitively, the larger the distance between $s_i$ and $z_t$ is, the lower the probability for the robot to measure $z_t$ if the it was at $s_i$. Note that this is only a simple option to approximate $p(z_t|s_i)$, and may not work too well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Particle():\n",
    "    def __init__(self, x, y, w):\n",
    "        self.state = np.array([x, y])\n",
    "        self.weight = w\n",
    "    \n",
    "    def predict(self, u_t):\n",
    "        motion_model_noise = np.random.normal(0, 0.01, size=self.state.shape)\n",
    "        self.state = self.state + u_t + motion_model_noise\n",
    "        \n",
    "    def update(self, z_t):\n",
    "        dist_from_state_to_z = np.linalg.norm(self.state - z_t)\n",
    "        self.weight = 1. / dist_from_state_to_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement the particle filter and run it for each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = 1000 # pick a number\n",
    "\n",
    "filtered_xs = []\n",
    "filtered_ys = []\n",
    "measurements = []\n",
    "particles_over_time = []\n",
    "\n",
    "# sample initial particles\n",
    "particles = []\n",
    "for i in range(num_particles):\n",
    "    x = np.random.uniform(0, 10)\n",
    "    y = np.random.uniform(0, 10)\n",
    "    w = 1\n",
    "    particles.append(Particle(x, y, w))\n",
    "\n",
    "# we assume constant control signal in this particular example\n",
    "u_t = np.array([10./num_data, 10./num_data]) \n",
    "    \n",
    "# run particle filter for each time step\n",
    "for i in range(num_data):\n",
    "    \n",
    "    # given u_t, move all the particles following the motion model\n",
    "    for p in particles:\n",
    "        p.predict(u_t)\n",
    "    \n",
    "    # get measurement z_t (in real life, get this from our sensor instead)\n",
    "    measurement_noise_x = np.random.normal(loc=0.0, scale=measurement_noise_x_var)\n",
    "    measurement_noise_y = np.random.normal(loc=0.0, scale=measurement_noise_y_var)\n",
    "    measurement_x_new = ground_truth_x[i+1] + measurement_noise_x\n",
    "    measurement_y_new = ground_truth_x[i+1] + measurement_noise_y\n",
    "    z_t = np.array([measurement_x_new, measurement_y_new])\n",
    "    measurements.append([measurement_x_new, measurement_y_new])\n",
    "    \n",
    "    # given z_t, update particles' weights\n",
    "    for p in particles:\n",
    "        p.update(z_t)\n",
    "    \n",
    "    # resample particles using the updated weights\n",
    "    alpha = 0.9\n",
    "    weights = [] \n",
    "    for p in particles:\n",
    "        weights.append(p.weight)\n",
    "    normalized_weights = np.array(weights) / float(np.sum(weights)) # array of normalized weights from each particle\n",
    "    indexes = np.random.choice(num_particles,\n",
    "                               size=num_particles,\n",
    "                               p=normalized_weights)\n",
    "    for j in range(int(num_particles * alpha)):\n",
    "        p = particles[indexes[j]]\n",
    "        particles[j] = p\n",
    "    for j in range(num_particles - int(num_particles * alpha)):\n",
    "        x = np.random.uniform(0,10)\n",
    "        y = np.random.uniform(0,10)\n",
    "        w = 0.\n",
    "        particles[int(num_particles * alpha) + j] = Particle(x,y,w)\n",
    "        \n",
    "    # get state estimate by taking the mean of the resampled particles (excluding the randomly sampled ones)\n",
    "    xs = []\n",
    "    ys = []\n",
    "    for p in particles[:int(num_particles * alpha)]:\n",
    "        xs.append(p.state[0])\n",
    "        ys.append(p.state[1])\n",
    "    estimated_x = np.mean(xs)\n",
    "    estimated_y = np.mean(ys)\n",
    "    \n",
    "    # store filtered state so we can plot them later\n",
    "    filtered_xs.append(estimated_x)\n",
    "    filtered_ys.append(estimated_y)\n",
    "    \n",
    "    # store resampled particles so we can plot them later\n",
    "    particles_over_time.append(copy.deepcopy(particles))\n",
    "measurements = np.array(measurements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how the particles evolve over time so we can get a sense on how particle filter works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = np.stack((ground_truth_x, ground_truth_y), axis=1)\n",
    "filtered_states = np.stack((filtered_xs, filtered_ys), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots = []\n",
    "\n",
    "t = 0\n",
    "for (set_of_ps, true_state, filtered_state, z_t) in tzip(particles_over_time, ground_truth[1:], filtered_states, measurements):\n",
    "    fig = plt.figure()\n",
    "    plt.scatter(true_state[0], true_state[1], color='blue', s=75)\n",
    "    plt.scatter(filtered_state[0], filtered_state[1], color='green', s=75)\n",
    "    plt.scatter(z_t[0], z_t[1], color='orange', s=75)\n",
    "    \n",
    "    for p in set_of_ps:\n",
    "        plt.scatter(p.state[0], p.state[1], color='red', s=p.weight * 25)\n",
    "    plt.title('time=%d, true (x,y)=(%.2f, %.2f) \\n filtered (x,y)=(%.2f, %.2f)' \n",
    "              % (t+1, true_state[0], true_state[1], filtered_state[0], filtered_state[1]))\n",
    "    plt.xlabel('x position')\n",
    "    plt.ylabel('y position')\n",
    "    plt.xlim(0,15)\n",
    "    plt.ylim(0,15)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    \n",
    "    blue_patch = patches.Patch(color='blue', label='true state')\n",
    "    red_patch = patches.Patch(color='red', label='weighted particles')\n",
    "    green_patch = patches.Patch(color='green', label='filtered state')\n",
    "    orange_patch = patches.Patch(color='orange', label='noisy measurement')\n",
    "    plt.legend(handles=[blue_patch, red_patch, green_patch, orange_patch], loc='upper left', fontsize=8)\n",
    "    \n",
    "    fig.canvas.draw()\n",
    "    plot_img = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
    "    plot_img = plot_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
    "    plots.append(plot_img)\n",
    "    t += 1\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for im in plots:\n",
    "    plt.figure(figsize=(14,14))\n",
    "    plt.imshow(im)\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    time.sleep(0.01)\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this if we want to save the plot animation as gif\n",
    "\n",
    "# import imageio\n",
    "# imageio.mimsave('pf.gif', plots)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the particles (i.e., red dots) that are closer to where the robot is (i.e., blue dot) are getting bigger over time (i.e., gain more weights) as the robot moves. Here, the state predicted by the particle filter is visualized as the green dot, while the sensor measurement is visualized as the orange dot.\n",
    "\n",
    "Notice that it takes some time for the particles to \"converge\" to the true state. This can be observed by looking at the poor performance of state estimation at earlier time steps. This is because initially the particles were spawned randomly so it will not produce a good estimation until we have enough measurements. Overall, the filter seems to work although not too well, let's plot the state estimation over time below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot the results\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(ground_truth[:,0], ground_truth[:,1])\n",
    "plt.plot(measurements[:,0], measurements[:,1])\n",
    "plt.plot(filtered_states[:,0], filtered_states[:,1])\n",
    "plt.xlabel('x position')\n",
    "plt.ylabel('y position')\n",
    "plt.legend(['ground truth trajectory', 'noisy measurements', 'particle filter estimate'])\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter seems to work as the green lines is close to the $x=y$ line, but not too good. What can we do to improve the performance?\n",
    "\n",
    "While having more particles corresponds to better approximation of the true distribution that we are trying to estimate, it makes computation to be more expensive, so this can be a limitation depending on the hardware used. \n",
    "\n",
    "Try experimenting with some of the parameters to see if you can improve things further. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
